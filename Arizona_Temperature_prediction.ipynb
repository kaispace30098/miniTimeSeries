{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOanJD+pHmWI0KapveuOy1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaispace30098/miniTimeSeries/blob/main/Arizona_Temperature_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "11ze2dvSF9Ww"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df0=pd.read_csv('/content/temp prediction.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df0.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl4ANOWlGey7",
        "outputId": "5dba08a6-9be7-43e5-cb53-2f6911537eeb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['year', 'day', 'temp'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df0.iloc[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk8dMO2qOU76",
        "outputId": "23957306-6fbd-4bfa-bdf2-256844881c49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "year    2022.0\n",
              "day      166.0\n",
              "temp      40.9\n",
              "Name: 1261, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to machine learning data frame for multi-output forecast\n",
        "days=len(df0)\n",
        "Tx=365\n",
        "Ty=3\n",
        "X=[]\n",
        "Y=[]\n",
        "for i in range(0,days-Tx-Ty+1):\n",
        "    tx=df0['temp'].iloc[i:i+Tx].tolist()\n",
        "    X.append(tx) \n",
        "    ty=df0['temp'].iloc[i+Tx:i+Tx+Ty].tolist() \n",
        "    Y.append(ty)\n",
        "X=np.array(X)\n",
        "X=X.reshape((X.shape[0],X.shape[1],1))\n",
        "Y=np.array(Y)\n"
      ],
      "metadata": {
        "id": "w8yYYr3tF_w1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.layers import Input,LSTM,Dense,GlobalAveragePooling1D"
      ],
      "metadata": {
        "id": "vA5zD6T0GpTP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=Input(shape=(Tx,1))\n",
        "x=LSTM(32,return_sequences=True,activation='tanh')(i)\n",
        "x=GlobalAveragePooling1D()(x)\n",
        "x=Dense(Ty)(x)\n",
        "\n",
        "model=Model(inputs=i,outputs=x)"
      ],
      "metadata": {
        "id": "gnOxJfWIGtF9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain=X[:-1]\n",
        "xtest=X[-1:]\n",
        "ytrain=Y[:-1]\n",
        "ytest=Y[-1:]\n"
      ],
      "metadata": {
        "id": "2D-OHSoOG4UP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mae', optimizer='adam')\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint('my_trained_model.h5',\n",
        "                                                    save_best_only=True,\n",
        "                                                    monitor='val_loss',\n",
        "                                                    verbose=1)\n",
        "callbacklist=[early_stopping,model_checkpoint]"
      ],
      "metadata": {
        "id": "1dyp6i10Gxiw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=300,callbacks=callbacklist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU_8zwOnHMoe",
        "outputId": "7ddc4f30-6246-4146-ae3d-72d91dcc7c40"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 30.6992\n",
            "Epoch 1: val_loss improved from inf to 40.26819, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 8s 71ms/step - loss: 30.6582 - val_loss: 40.2682\n",
            "Epoch 2/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 29.4868\n",
            "Epoch 2: val_loss improved from 40.26819 to 38.70186, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 29.4868 - val_loss: 38.7019\n",
            "Epoch 3/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 27.5654\n",
            "Epoch 3: val_loss improved from 38.70186 to 36.65860, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 27.5654 - val_loss: 36.6586\n",
            "Epoch 4/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 25.8027\n",
            "Epoch 4: val_loss improved from 36.65860 to 35.19606, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 25.8027 - val_loss: 35.1961\n",
            "Epoch 5/300\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 24.2270\n",
            "Epoch 5: val_loss improved from 35.19606 to 33.34230, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 24.2343 - val_loss: 33.3423\n",
            "Epoch 6/300\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 22.3391\n",
            "Epoch 6: val_loss improved from 33.34230 to 31.65275, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 22.3544 - val_loss: 31.6527\n",
            "Epoch 7/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 20.8539\n",
            "Epoch 7: val_loss improved from 31.65275 to 30.33449, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 20.8760 - val_loss: 30.3345\n",
            "Epoch 8/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 19.6439\n",
            "Epoch 8: val_loss improved from 30.33449 to 29.14283, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 19.6384 - val_loss: 29.1428\n",
            "Epoch 9/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 18.6997\n",
            "Epoch 9: val_loss improved from 29.14283 to 28.01681, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 18.4911 - val_loss: 28.0168\n",
            "Epoch 10/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 17.3831\n",
            "Epoch 10: val_loss improved from 28.01681 to 26.93116, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 17.4030 - val_loss: 26.9312\n",
            "Epoch 11/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 16.5197\n",
            "Epoch 11: val_loss improved from 26.93116 to 25.88455, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 16.3599 - val_loss: 25.8846\n",
            "Epoch 12/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 15.3314\n",
            "Epoch 12: val_loss improved from 25.88455 to 24.87802, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 15.3718 - val_loss: 24.8780\n",
            "Epoch 13/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 14.5797\n",
            "Epoch 13: val_loss improved from 24.87802 to 23.89837, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 14.4464 - val_loss: 23.8984\n",
            "Epoch 14/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 13.6463\n",
            "Epoch 14: val_loss improved from 23.89837 to 22.95892, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 13.5757 - val_loss: 22.9589\n",
            "Epoch 15/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 12.7683\n",
            "Epoch 15: val_loss improved from 22.95892 to 22.06824, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 12.7799 - val_loss: 22.0682\n",
            "Epoch 16/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 12.1199\n",
            "Epoch 16: val_loss improved from 22.06824 to 21.21768, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 12.0647 - val_loss: 21.2177\n",
            "Epoch 17/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 11.4410\n",
            "Epoch 17: val_loss improved from 21.21768 to 20.41889, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 11.4296 - val_loss: 20.4189\n",
            "Epoch 18/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 10.9929\n",
            "Epoch 18: val_loss improved from 20.41889 to 19.68226, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 10.8752 - val_loss: 19.6823\n",
            "Epoch 19/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 10.4609\n",
            "Epoch 19: val_loss improved from 19.68226 to 19.00171, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 10.4092 - val_loss: 19.0017\n",
            "Epoch 20/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 9.9384\n",
            "Epoch 20: val_loss improved from 19.00171 to 18.36299, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 10.0132 - val_loss: 18.3630\n",
            "Epoch 21/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 9.6910\n",
            "Epoch 21: val_loss improved from 18.36299 to 17.77885, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 9.6749 - val_loss: 17.7788\n",
            "Epoch 22/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 9.4241\n",
            "Epoch 22: val_loss improved from 17.77885 to 17.22190, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 9.3951 - val_loss: 17.2219\n",
            "Epoch 23/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 9.2402\n",
            "Epoch 23: val_loss improved from 17.22190 to 16.71100, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 9.1534 - val_loss: 16.7110\n",
            "Epoch 24/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 8.9995\n",
            "Epoch 24: val_loss improved from 16.71100 to 16.21227, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 8.9428 - val_loss: 16.2123\n",
            "Epoch 25/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 8.6947\n",
            "Epoch 25: val_loss improved from 16.21227 to 15.74849, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 8.7297 - val_loss: 15.7485\n",
            "Epoch 26/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 8.4708\n",
            "Epoch 26: val_loss improved from 15.74849 to 15.30174, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 8.5635 - val_loss: 15.3017\n",
            "Epoch 27/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 8.4456\n",
            "Epoch 27: val_loss improved from 15.30174 to 14.85053, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 8.3752 - val_loss: 14.8505\n",
            "Epoch 28/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 8.2247\n",
            "Epoch 28: val_loss improved from 14.85053 to 14.44382, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 8.1917 - val_loss: 14.4438\n",
            "Epoch 29/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 8.2172\n",
            "Epoch 29: val_loss improved from 14.44382 to 14.07165, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 8.1813 - val_loss: 14.0717\n",
            "Epoch 30/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 8.0517\n",
            "Epoch 30: val_loss improved from 14.07165 to 13.71535, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 8.0075 - val_loss: 13.7154\n",
            "Epoch 31/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 7.7603\n",
            "Epoch 31: val_loss improved from 13.71535 to 13.31869, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.6772 - val_loss: 13.3187\n",
            "Epoch 32/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 7.3980\n",
            "Epoch 32: val_loss improved from 13.31869 to 12.81132, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.3775 - val_loss: 12.8113\n",
            "Epoch 33/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 7.2499\n",
            "Epoch 33: val_loss did not improve from 12.81132\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.3199 - val_loss: 14.5399\n",
            "Epoch 34/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 7.5844\n",
            "Epoch 34: val_loss did not improve from 12.81132\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.5809 - val_loss: 13.3415\n",
            "Epoch 35/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 6.9982\n",
            "Epoch 35: val_loss improved from 12.81132 to 12.78354, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.0183 - val_loss: 12.7835\n",
            "Epoch 36/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 6.7536\n",
            "Epoch 36: val_loss improved from 12.78354 to 12.24066, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.8321 - val_loss: 12.2407\n",
            "Epoch 37/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 6.8091\n",
            "Epoch 37: val_loss improved from 12.24066 to 11.82687, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.8092 - val_loss: 11.8269\n",
            "Epoch 38/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 6.3971\n",
            "Epoch 38: val_loss improved from 11.82687 to 11.27640, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 6.3855 - val_loss: 11.2764\n",
            "Epoch 39/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 6.5361\n",
            "Epoch 39: val_loss improved from 11.27640 to 10.91549, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.4639 - val_loss: 10.9155\n",
            "Epoch 40/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.9882\n",
            "Epoch 40: val_loss improved from 10.91549 to 10.44420, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.9962 - val_loss: 10.4442\n",
            "Epoch 41/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.7122\n",
            "Epoch 41: val_loss improved from 10.44420 to 10.03937, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.7158 - val_loss: 10.0394\n",
            "Epoch 42/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.5213\n",
            "Epoch 42: val_loss improved from 10.03937 to 9.55185, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 5.5491 - val_loss: 9.5519\n",
            "Epoch 43/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.4864\n",
            "Epoch 43: val_loss improved from 9.55185 to 9.13583, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.5102 - val_loss: 9.1358\n",
            "Epoch 44/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 5.9772\n",
            "Epoch 44: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 6.3772 - val_loss: 19.1934\n",
            "Epoch 45/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 9.9082\n",
            "Epoch 45: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 9.8352 - val_loss: 16.9883\n",
            "Epoch 46/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 9.0267\n",
            "Epoch 46: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 8.9028 - val_loss: 15.7576\n",
            "Epoch 47/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 8.2381\n",
            "Epoch 47: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 8.2898 - val_loss: 13.6946\n",
            "Epoch 48/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 7.8623\n",
            "Epoch 48: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.8418 - val_loss: 11.7030\n",
            "Epoch 49/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 7.6251\n",
            "Epoch 49: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.6237 - val_loss: 10.7644\n",
            "Epoch 50/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 7.5108\n",
            "Epoch 50: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.5142 - val_loss: 10.4714\n",
            "Epoch 51/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 7.4818\n",
            "Epoch 51: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.4446 - val_loss: 10.2943\n",
            "Epoch 52/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 7.4521\n",
            "Epoch 52: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 7.3910 - val_loss: 10.8804\n",
            "Epoch 53/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 7.3327\n",
            "Epoch 53: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.3026 - val_loss: 10.0004\n",
            "Epoch 54/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 7.1861\n",
            "Epoch 54: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.2428 - val_loss: 11.9598\n",
            "Epoch 55/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 7.4396\n",
            "Epoch 55: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 7.4115 - val_loss: 11.5207\n",
            "Epoch 56/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 7.3248\n",
            "Epoch 56: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.2475 - val_loss: 11.4317\n",
            "Epoch 57/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 7.5781\n",
            "Epoch 57: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.5720 - val_loss: 11.3324\n",
            "Epoch 58/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 7.5049\n",
            "Epoch 58: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.5189 - val_loss: 11.1398\n",
            "Epoch 59/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 7.4515\n",
            "Epoch 59: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.4508 - val_loss: 10.9328\n",
            "Epoch 60/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 7.4068\n",
            "Epoch 60: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.3995 - val_loss: 10.7168\n",
            "Epoch 61/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 7.2887\n",
            "Epoch 61: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.2720 - val_loss: 10.4627\n",
            "Epoch 62/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 7.1321\n",
            "Epoch 62: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.1717 - val_loss: 10.2272\n",
            "Epoch 63/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 7.3013\n",
            "Epoch 63: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.2863 - val_loss: 10.0907\n",
            "Epoch 64/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 7.1003\n",
            "Epoch 64: val_loss did not improve from 9.13583\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 7.1733 - val_loss: 10.5530\n",
            "Epoch 65/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 6.5798\n",
            "Epoch 65: val_loss improved from 9.13583 to 7.68313, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 6.4957 - val_loss: 7.6831\n",
            "Epoch 66/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 6.3887\n",
            "Epoch 66: val_loss improved from 7.68313 to 7.45150, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.3494 - val_loss: 7.4515\n",
            "Epoch 67/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 6.8478\n",
            "Epoch 67: val_loss did not improve from 7.45150\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.7484 - val_loss: 9.3282\n",
            "Epoch 68/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 6.6507\n",
            "Epoch 68: val_loss did not improve from 7.45150\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.6618 - val_loss: 9.0609\n",
            "Epoch 69/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 6.6599\n",
            "Epoch 69: val_loss did not improve from 7.45150\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 6.7063 - val_loss: 8.7989\n",
            "Epoch 70/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 6.4483\n",
            "Epoch 70: val_loss did not improve from 7.45150\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.5057 - val_loss: 9.1991\n",
            "Epoch 71/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 6.3446\n",
            "Epoch 71: val_loss did not improve from 7.45150\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.2893 - val_loss: 8.1048\n",
            "Epoch 72/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 5.8707\n",
            "Epoch 72: val_loss did not improve from 7.45150\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 5.8813 - val_loss: 7.6728\n",
            "Epoch 73/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 5.7643\n",
            "Epoch 73: val_loss improved from 7.45150 to 7.20363, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 5.8526 - val_loss: 7.2036\n",
            "Epoch 74/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.5653\n",
            "Epoch 74: val_loss improved from 7.20363 to 6.74353, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.5297 - val_loss: 6.7435\n",
            "Epoch 75/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 5.3912\n",
            "Epoch 75: val_loss improved from 6.74353 to 6.50894, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.3634 - val_loss: 6.5089\n",
            "Epoch 76/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.7549\n",
            "Epoch 76: val_loss did not improve from 6.50894\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.8945 - val_loss: 9.2054\n",
            "Epoch 77/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 6.5198\n",
            "Epoch 77: val_loss did not improve from 6.50894\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.4843 - val_loss: 8.8697\n",
            "Epoch 78/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 6.4499\n",
            "Epoch 78: val_loss did not improve from 6.50894\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 6.4499 - val_loss: 8.5023\n",
            "Epoch 79/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 6.3069\n",
            "Epoch 79: val_loss did not improve from 6.50894\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.2949 - val_loss: 8.1378\n",
            "Epoch 80/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 6.2154\n",
            "Epoch 80: val_loss did not improve from 6.50894\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 6.2024 - val_loss: 7.7451\n",
            "Epoch 81/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.9883\n",
            "Epoch 81: val_loss did not improve from 6.50894\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.0454 - val_loss: 7.3597\n",
            "Epoch 82/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 5.9241\n",
            "Epoch 82: val_loss did not improve from 6.50894\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.9072 - val_loss: 6.9584\n",
            "Epoch 83/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.4861\n",
            "Epoch 83: val_loss improved from 6.50894 to 5.91476, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.4390 - val_loss: 5.9148\n",
            "Epoch 84/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 5.1685\n",
            "Epoch 84: val_loss improved from 5.91476 to 5.03617, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 5.1685 - val_loss: 5.0362\n",
            "Epoch 85/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.7772\n",
            "Epoch 85: val_loss improved from 5.03617 to 3.88984, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.7417 - val_loss: 3.8898\n",
            "Epoch 86/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.5449\n",
            "Epoch 86: val_loss improved from 3.88984 to 3.61121, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.5313 - val_loss: 3.6112\n",
            "Epoch 87/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.4752\n",
            "Epoch 87: val_loss improved from 3.61121 to 3.38544, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.4564 - val_loss: 3.3854\n",
            "Epoch 88/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.8234\n",
            "Epoch 88: val_loss did not improve from 3.38544\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 4.7736 - val_loss: 4.2762\n",
            "Epoch 89/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.3487\n",
            "Epoch 89: val_loss did not improve from 3.38544\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.3482 - val_loss: 4.0750\n",
            "Epoch 90/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.1915\n",
            "Epoch 90: val_loss did not improve from 3.38544\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.1967 - val_loss: 3.7277\n",
            "Epoch 91/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.1724\n",
            "Epoch 91: val_loss improved from 3.38544 to 2.60618, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.2069 - val_loss: 2.6062\n",
            "Epoch 92/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 4.3262\n",
            "Epoch 92: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.3731 - val_loss: 2.9973\n",
            "Epoch 93/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.2393\n",
            "Epoch 93: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.2917 - val_loss: 2.7646\n",
            "Epoch 94/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.2988\n",
            "Epoch 94: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.2988 - val_loss: 5.6180\n",
            "Epoch 95/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.6228\n",
            "Epoch 95: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.6541 - val_loss: 5.4649\n",
            "Epoch 96/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.3358\n",
            "Epoch 96: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.3195 - val_loss: 3.9000\n",
            "Epoch 97/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.2035\n",
            "Epoch 97: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.2035 - val_loss: 3.9442\n",
            "Epoch 98/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.2148\n",
            "Epoch 98: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.1701 - val_loss: 3.2465\n",
            "Epoch 99/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.0326\n",
            "Epoch 99: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.0161 - val_loss: 3.2191\n",
            "Epoch 100/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.1019\n",
            "Epoch 100: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.1019 - val_loss: 4.2433\n",
            "Epoch 101/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.2007\n",
            "Epoch 101: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.1781 - val_loss: 2.7376\n",
            "Epoch 102/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.4512\n",
            "Epoch 102: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.4292 - val_loss: 2.7172\n",
            "Epoch 103/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 4.0554\n",
            "Epoch 103: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.2245 - val_loss: 2.8729\n",
            "Epoch 104/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.9110\n",
            "Epoch 104: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.0270 - val_loss: 6.5835\n",
            "Epoch 105/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.4534\n",
            "Epoch 105: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.3781 - val_loss: 6.4775\n",
            "Epoch 106/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.3867\n",
            "Epoch 106: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.4026 - val_loss: 4.0784\n",
            "Epoch 107/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 5.2746\n",
            "Epoch 107: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.1484 - val_loss: 4.8572\n",
            "Epoch 108/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.6763\n",
            "Epoch 108: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.6763 - val_loss: 4.4060\n",
            "Epoch 109/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.5668\n",
            "Epoch 109: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.5668 - val_loss: 4.1607\n",
            "Epoch 110/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.3720\n",
            "Epoch 110: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.3834 - val_loss: 4.1636\n",
            "Epoch 111/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.4211\n",
            "Epoch 111: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.4150 - val_loss: 3.9730\n",
            "Epoch 112/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.4201\n",
            "Epoch 112: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.5245 - val_loss: 4.0137\n",
            "Epoch 113/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.6622\n",
            "Epoch 113: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.6622 - val_loss: 4.2066\n",
            "Epoch 114/300\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 5.4049\n",
            "Epoch 114: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 5.4031 - val_loss: 6.5290\n",
            "Epoch 115/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.4154\n",
            "Epoch 115: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 5.3208 - val_loss: 4.7923\n",
            "Epoch 116/300\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 4.7973\n",
            "Epoch 116: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 4.7699 - val_loss: 4.9850\n",
            "Epoch 117/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 4.4155\n",
            "Epoch 117: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.4656 - val_loss: 3.5467\n",
            "Epoch 118/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.4568\n",
            "Epoch 118: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.4251 - val_loss: 4.6704\n",
            "Epoch 119/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.7059\n",
            "Epoch 119: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.6962 - val_loss: 3.4216\n",
            "Epoch 120/300\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 4.4741\n",
            "Epoch 120: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.4920 - val_loss: 4.6855\n",
            "Epoch 121/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.2987\n",
            "Epoch 121: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.2987 - val_loss: 3.7550\n",
            "Epoch 122/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 4.5687\n",
            "Epoch 122: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.5730 - val_loss: 3.7808\n",
            "Epoch 123/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.2455\n",
            "Epoch 123: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.3092 - val_loss: 4.2107\n",
            "Epoch 124/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.4324\n",
            "Epoch 124: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.4026 - val_loss: 3.9348\n",
            "Epoch 125/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.3600\n",
            "Epoch 125: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.5616 - val_loss: 4.1410\n",
            "Epoch 126/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 5.9930\n",
            "Epoch 126: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.0705 - val_loss: 7.0863\n",
            "Epoch 127/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 6.0376\n",
            "Epoch 127: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 6.0343 - val_loss: 6.6326\n",
            "Epoch 128/300\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 5.5406\n",
            "Epoch 128: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 5.5053 - val_loss: 6.2016\n",
            "Epoch 129/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.4116\n",
            "Epoch 129: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.3910 - val_loss: 4.4302\n",
            "Epoch 130/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.3648\n",
            "Epoch 130: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.3530 - val_loss: 4.1001\n",
            "Epoch 131/300\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 4.4590\n",
            "Epoch 131: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.4039 - val_loss: 3.9227\n",
            "Epoch 132/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.4867\n",
            "Epoch 132: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.5022 - val_loss: 4.4328\n",
            "Epoch 133/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.3339\n",
            "Epoch 133: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.3339 - val_loss: 3.3696\n",
            "Epoch 134/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 4.4090\n",
            "Epoch 134: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.4486 - val_loss: 4.4638\n",
            "Epoch 135/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 4.7426\n",
            "Epoch 135: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.7758 - val_loss: 4.7675\n",
            "Epoch 136/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 4.7179\n",
            "Epoch 136: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.7955 - val_loss: 4.3896\n",
            "Epoch 137/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.8928\n",
            "Epoch 137: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.8876 - val_loss: 5.0379\n",
            "Epoch 138/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.6686\n",
            "Epoch 138: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.6691 - val_loss: 4.8359\n",
            "Epoch 139/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.2700\n",
            "Epoch 139: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.2700 - val_loss: 4.4628\n",
            "Epoch 140/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.1852\n",
            "Epoch 140: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.1852 - val_loss: 3.3416\n",
            "Epoch 141/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.1916\n",
            "Epoch 141: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.1844 - val_loss: 3.2416\n",
            "Epoch 142/300\n",
            "28/28 [==============================] - ETA: 0s - loss: 4.2172\n",
            "Epoch 142: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.2172 - val_loss: 4.9043\n",
            "Epoch 143/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.5098\n",
            "Epoch 143: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.5637 - val_loss: 4.0691\n",
            "Epoch 144/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 5.0279\n",
            "Epoch 144: val_loss did not improve from 2.60618\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 5.0263 - val_loss: 5.3675\n",
            "Epoch 145/300\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 4.9204\n",
            "Epoch 145: val_loss improved from 2.60618 to 0.75362, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.9510 - val_loss: 0.7536\n",
            "Epoch 146/300\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 4.8405\n",
            "Epoch 146: val_loss improved from 0.75362 to 0.71766, saving model to my_trained_model.h5\n",
            "28/28 [==============================] - 0s 14ms/step - loss: 4.8093 - val_loss: 0.7177\n",
            "Epoch 147/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 4.3118\n",
            "Epoch 147: val_loss did not improve from 0.71766\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.3341 - val_loss: 1.8562\n",
            "Epoch 148/300\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 4.4296\n",
            "Epoch 148: val_loss did not improve from 0.71766\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.4328 - val_loss: 3.3492\n",
            "Epoch 149/300\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 4.4519\n",
            "Epoch 149: val_loss did not improve from 0.71766\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 4.4169 - val_loss: 3.6514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "cvwKAJNVHldo",
        "outputId": "da6e68ae-2610-4e70-f00e-3cc5ba62e15f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f721a1ce490>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgfs3kmYiZkK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###PREDICTION iNPUT\n",
        "input=xtest[0][Ty:]\n",
        "a=input.reshape(input.shape[0]).tolist()\n",
        "for i in Y[-1].tolist():\n",
        "  a.append(i)\n",
        "\n",
        "input=np.reshape(np.array(a),(1,np.array(a).shape[0],1))"
      ],
      "metadata": {
        "id": "Ww4pBGvJIJ4-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "loadmodel=load_model(\"/content/my_trained_model.h5\")\n",
        "prediction=loadmodel.predict(input)\n",
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQvWuO60rr3O",
        "outputId": "714c8f3d-cfc5-49af-a685-331495d7c3ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[40.18931 , 40.149395, 40.407494]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "i=Input(shape=(Tx,1))\n",
        "x=Bidirectional(LSTM(32,return_sequences=True,activation='tanh'))(i)\n",
        "x=GlobalAveragePooling1D()(x)\n",
        "x=Dense(Ty)(x)\n",
        "\n",
        "model2=Model(inputs=i,outputs=x)"
      ],
      "metadata": {
        "id": "MJaLXR81KRWt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(loss='mae', optimizer='adam')\n",
        "history=model2.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHdWCCXxMZ9Q",
        "outputId": "9e67b59b-a755-46ad-f737-802f2085a996"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "28/28 [==============================] - 4s 53ms/step - loss: 30.6933 - val_loss: 40.0632\n",
            "Epoch 2/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 29.0779 - val_loss: 38.1014\n",
            "Epoch 3/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 26.5508 - val_loss: 35.0314\n",
            "Epoch 4/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 23.3221 - val_loss: 31.8323\n",
            "Epoch 5/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 20.2340 - val_loss: 28.8301\n",
            "Epoch 6/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 17.4268 - val_loss: 26.0592\n",
            "Epoch 7/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 14.6037 - val_loss: 23.1093\n",
            "Epoch 8/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 12.2252 - val_loss: 20.7243\n",
            "Epoch 9/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 10.6731 - val_loss: 18.9281\n",
            "Epoch 10/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 9.7229 - val_loss: 17.4520\n",
            "Epoch 11/300\n",
            "28/28 [==============================] - 1s 33ms/step - loss: 9.0959 - val_loss: 16.2744\n",
            "Epoch 12/300\n",
            "28/28 [==============================] - 1s 34ms/step - loss: 8.6594 - val_loss: 15.2506\n",
            "Epoch 13/300\n",
            "28/28 [==============================] - 1s 34ms/step - loss: 8.3415 - val_loss: 14.3345\n",
            "Epoch 14/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 8.0288 - val_loss: 13.5437\n",
            "Epoch 15/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 7.5377 - val_loss: 12.7230\n",
            "Epoch 16/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 7.2919 - val_loss: 14.3259\n",
            "Epoch 17/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 6.8141 - val_loss: 10.9080\n",
            "Epoch 18/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 6.2238 - val_loss: 10.1248\n",
            "Epoch 19/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.9437 - val_loss: 9.5114\n",
            "Epoch 20/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.3375 - val_loss: 8.6577\n",
            "Epoch 21/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.9802 - val_loss: 7.7668\n",
            "Epoch 22/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.5601 - val_loss: 6.8054\n",
            "Epoch 23/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.2430 - val_loss: 5.8396\n",
            "Epoch 24/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.5259 - val_loss: 5.5456\n",
            "Epoch 25/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.7989 - val_loss: 7.8183\n",
            "Epoch 26/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.4793 - val_loss: 7.7623\n",
            "Epoch 27/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.9183 - val_loss: 8.9141\n",
            "Epoch 28/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.3652 - val_loss: 5.8812\n",
            "Epoch 29/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.0417 - val_loss: 4.8024\n",
            "Epoch 30/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.8849 - val_loss: 4.3862\n",
            "Epoch 31/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.7835 - val_loss: 3.7916\n",
            "Epoch 32/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 3.9857 - val_loss: 2.8261\n",
            "Epoch 33/300\n",
            "28/28 [==============================] - 1s 37ms/step - loss: 3.6864 - val_loss: 5.4704\n",
            "Epoch 34/300\n",
            "28/28 [==============================] - 1s 35ms/step - loss: 3.6684 - val_loss: 4.8689\n",
            "Epoch 35/300\n",
            "28/28 [==============================] - 1s 32ms/step - loss: 3.8626 - val_loss: 3.6323\n",
            "Epoch 36/300\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 4.5096 - val_loss: 3.7615\n",
            "Epoch 37/300\n",
            "28/28 [==============================] - 1s 41ms/step - loss: 4.0337 - val_loss: 1.9842\n",
            "Epoch 38/300\n",
            "28/28 [==============================] - 1s 37ms/step - loss: 4.5063 - val_loss: 4.7854\n",
            "Epoch 39/300\n",
            "28/28 [==============================] - 1s 37ms/step - loss: 3.6856 - val_loss: 4.1982\n",
            "Epoch 40/300\n",
            "28/28 [==============================] - 1s 46ms/step - loss: 3.7586 - val_loss: 3.9927\n",
            "Epoch 41/300\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 3.5325 - val_loss: 2.3229\n",
            "Epoch 42/300\n",
            "28/28 [==============================] - 1s 36ms/step - loss: 3.2776 - val_loss: 4.0706\n",
            "Epoch 43/300\n",
            "28/28 [==============================] - 1s 39ms/step - loss: 3.7594 - val_loss: 1.9540\n",
            "Epoch 44/300\n",
            "28/28 [==============================] - 1s 33ms/step - loss: 3.4678 - val_loss: 1.4184\n",
            "Epoch 45/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.5772 - val_loss: 1.6484\n",
            "Epoch 46/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4973 - val_loss: 1.7391\n",
            "Epoch 47/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.3825 - val_loss: 2.2599\n",
            "Epoch 48/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.3138 - val_loss: 1.9906\n",
            "Epoch 49/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.3336 - val_loss: 1.3723\n",
            "Epoch 50/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4259 - val_loss: 1.1860\n",
            "Epoch 51/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.6890 - val_loss: 1.0483\n",
            "Epoch 52/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.3660 - val_loss: 2.7528\n",
            "Epoch 53/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.8453 - val_loss: 8.7656\n",
            "Epoch 54/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.3122 - val_loss: 2.7932\n",
            "Epoch 55/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.5480 - val_loss: 2.4926\n",
            "Epoch 56/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.9118 - val_loss: 1.5624\n",
            "Epoch 57/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4523 - val_loss: 1.2995\n",
            "Epoch 58/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.3454 - val_loss: 0.8073\n",
            "Epoch 59/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.2206 - val_loss: 0.7796\n",
            "Epoch 60/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.2459 - val_loss: 1.8505\n",
            "Epoch 61/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.1915 - val_loss: 0.6727\n",
            "Epoch 62/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.2208 - val_loss: 1.9655\n",
            "Epoch 63/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.1299 - val_loss: 0.6215\n",
            "Epoch 64/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.0885 - val_loss: 0.7851\n",
            "Epoch 65/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6681 - val_loss: 3.6477\n",
            "Epoch 66/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.5618 - val_loss: 5.1119\n",
            "Epoch 67/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.9805 - val_loss: 4.0251\n",
            "Epoch 68/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.8303 - val_loss: 3.2605\n",
            "Epoch 69/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.4781 - val_loss: 1.7552\n",
            "Epoch 70/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 3.2340 - val_loss: 1.2955\n",
            "Epoch 71/300\n",
            "28/28 [==============================] - 1s 48ms/step - loss: 3.4712 - val_loss: 1.5027\n",
            "Epoch 72/300\n",
            "28/28 [==============================] - 1s 37ms/step - loss: 4.8855 - val_loss: 5.0042\n",
            "Epoch 73/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.3147 - val_loss: 8.4525\n",
            "Epoch 74/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.0217 - val_loss: 5.8085\n",
            "Epoch 75/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.4657 - val_loss: 5.7007\n",
            "Epoch 76/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 4.1307 - val_loss: 5.1074\n",
            "Epoch 77/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.1625 - val_loss: 4.1718\n",
            "Epoch 78/300\n",
            "28/28 [==============================] - 1s 41ms/step - loss: 4.4787 - val_loss: 4.1546\n",
            "Epoch 79/300\n",
            "28/28 [==============================] - 1s 53ms/step - loss: 3.6503 - val_loss: 2.8321\n",
            "Epoch 80/300\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 3.3958 - val_loss: 2.0322\n",
            "Epoch 81/300\n",
            "28/28 [==============================] - 2s 71ms/step - loss: 3.4524 - val_loss: 3.3362\n",
            "Epoch 82/300\n",
            "28/28 [==============================] - 1s 40ms/step - loss: 3.9570 - val_loss: 2.7693\n",
            "Epoch 83/300\n",
            "28/28 [==============================] - 1s 40ms/step - loss: 3.6408 - val_loss: 2.2806\n",
            "Epoch 84/300\n",
            "28/28 [==============================] - 1s 35ms/step - loss: 3.4442 - val_loss: 1.1316\n",
            "Epoch 85/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.5348 - val_loss: 1.8600\n",
            "Epoch 86/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.8749 - val_loss: 3.1194\n",
            "Epoch 87/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6526 - val_loss: 2.4071\n",
            "Epoch 88/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.5626 - val_loss: 2.5147\n",
            "Epoch 89/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4970 - val_loss: 1.1944\n",
            "Epoch 90/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6957 - val_loss: 0.8222\n",
            "Epoch 91/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.5163 - val_loss: 1.2696\n",
            "Epoch 92/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.5559 - val_loss: 1.6197\n",
            "Epoch 93/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.3411 - val_loss: 0.7929\n",
            "Epoch 94/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4274 - val_loss: 0.8941\n",
            "Epoch 95/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4506 - val_loss: 1.5117\n",
            "Epoch 96/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.0145 - val_loss: 2.5546\n",
            "Epoch 97/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.5597 - val_loss: 1.4009\n",
            "Epoch 98/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4947 - val_loss: 1.5613\n",
            "Epoch 99/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4138 - val_loss: 1.7743\n",
            "Epoch 100/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.4823 - val_loss: 2.2064\n",
            "Epoch 101/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.1394 - val_loss: 1.1457\n",
            "Epoch 102/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.5119 - val_loss: 4.1925\n",
            "Epoch 103/300\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 4.0429 - val_loss: 3.4682\n",
            "Epoch 104/300\n",
            "28/28 [==============================] - 1s 34ms/step - loss: 3.6619 - val_loss: 1.9853\n",
            "Epoch 105/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.5626 - val_loss: 2.1404\n",
            "Epoch 106/300\n",
            "28/28 [==============================] - 1s 35ms/step - loss: 4.6362 - val_loss: 5.6330\n",
            "Epoch 107/300\n",
            "28/28 [==============================] - 1s 41ms/step - loss: 4.6386 - val_loss: 5.0108\n",
            "Epoch 108/300\n",
            "28/28 [==============================] - 1s 40ms/step - loss: 4.9610 - val_loss: 2.0324\n",
            "Epoch 109/300\n",
            "28/28 [==============================] - 1s 30ms/step - loss: 4.6990 - val_loss: 5.1773\n",
            "Epoch 110/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 5.6357 - val_loss: 5.7357\n",
            "Epoch 111/300\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 5.8344 - val_loss: 6.6192\n",
            "Epoch 112/300\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 5.4255 - val_loss: 5.8573\n",
            "Epoch 113/300\n",
            "28/28 [==============================] - 1s 36ms/step - loss: 4.2486 - val_loss: 2.5457\n",
            "Epoch 114/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.5918 - val_loss: 0.7894\n",
            "Epoch 115/300\n",
            "28/28 [==============================] - 1s 34ms/step - loss: 3.2197 - val_loss: 1.0292\n",
            "Epoch 116/300\n",
            "28/28 [==============================] - 1s 31ms/step - loss: 3.2139 - val_loss: 0.7276\n",
            "Epoch 117/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.1717 - val_loss: 0.6780\n",
            "Epoch 118/300\n",
            "28/28 [==============================] - 1s 38ms/step - loss: 3.0842 - val_loss: 0.6524\n",
            "Epoch 119/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 3.1054 - val_loss: 0.7855\n",
            "Epoch 120/300\n",
            "28/28 [==============================] - 1s 30ms/step - loss: 3.5173 - val_loss: 1.4076\n",
            "Epoch 121/300\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 3.5476 - val_loss: 0.5784\n",
            "Epoch 122/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6000 - val_loss: 1.8466\n",
            "Epoch 123/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.0233 - val_loss: 2.7356\n",
            "Epoch 124/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.9490 - val_loss: 2.3065\n",
            "Epoch 125/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.8223 - val_loss: 1.9582\n",
            "Epoch 126/300\n",
            "28/28 [==============================] - 1s 30ms/step - loss: 4.0718 - val_loss: 2.8897\n",
            "Epoch 127/300\n",
            "28/28 [==============================] - 1s 31ms/step - loss: 3.6466 - val_loss: 1.4085\n",
            "Epoch 128/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.9744 - val_loss: 2.2840\n",
            "Epoch 129/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.9255 - val_loss: 3.3907\n",
            "Epoch 130/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.5484 - val_loss: 2.0020\n",
            "Epoch 131/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.7104 - val_loss: 0.6623\n",
            "Epoch 132/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.0228 - val_loss: 2.2642\n",
            "Epoch 133/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.5936 - val_loss: 0.9750\n",
            "Epoch 134/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.5666 - val_loss: 0.7453\n",
            "Epoch 135/300\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 3.6865 - val_loss: 0.7315\n",
            "Epoch 136/300\n",
            "28/28 [==============================] - 1s 40ms/step - loss: 7.4349 - val_loss: 8.9652\n",
            "Epoch 137/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 7.0366 - val_loss: 8.3230\n",
            "Epoch 138/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 6.3046 - val_loss: 6.9829\n",
            "Epoch 139/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.6108 - val_loss: 5.3101\n",
            "Epoch 140/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.6481 - val_loss: 1.7657\n",
            "Epoch 141/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.3669 - val_loss: 4.7858\n",
            "Epoch 142/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.9046 - val_loss: 1.6166\n",
            "Epoch 143/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6744 - val_loss: 1.6369\n",
            "Epoch 144/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.9635 - val_loss: 1.9382\n",
            "Epoch 145/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.9151 - val_loss: 2.1899\n",
            "Epoch 146/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.9091 - val_loss: 4.0230\n",
            "Epoch 147/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 6.2582 - val_loss: 8.1607\n",
            "Epoch 148/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 5.3759 - val_loss: 1.7870\n",
            "Epoch 149/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.5205 - val_loss: 2.8189\n",
            "Epoch 150/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.4819 - val_loss: 1.9230\n",
            "Epoch 151/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.3345 - val_loss: 1.4715\n",
            "Epoch 152/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.2378 - val_loss: 1.0868\n",
            "Epoch 153/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.5278 - val_loss: 0.9613\n",
            "Epoch 154/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.3759 - val_loss: 0.7980\n",
            "Epoch 155/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 3.3674 - val_loss: 0.5656\n",
            "Epoch 156/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 3.3638 - val_loss: 0.6950\n",
            "Epoch 157/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.3827 - val_loss: 4.9660\n",
            "Epoch 158/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.1049 - val_loss: 4.7850\n",
            "Epoch 159/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.7347 - val_loss: 2.6118\n",
            "Epoch 160/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6381 - val_loss: 2.3811\n",
            "Epoch 161/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4682 - val_loss: 1.6712\n",
            "Epoch 162/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 3.6443 - val_loss: 2.4863\n",
            "Epoch 163/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.7026 - val_loss: 2.7097\n",
            "Epoch 164/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.7764 - val_loss: 3.2720\n",
            "Epoch 165/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.8278 - val_loss: 4.2622\n",
            "Epoch 166/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.4150 - val_loss: 2.6256\n",
            "Epoch 167/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.2286 - val_loss: 1.1881\n",
            "Epoch 168/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.8806 - val_loss: 3.8930\n",
            "Epoch 169/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.2623 - val_loss: 4.8213\n",
            "Epoch 170/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.1061 - val_loss: 3.5633\n",
            "Epoch 171/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.7859 - val_loss: 0.7182\n",
            "Epoch 172/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.6355 - val_loss: 2.3102\n",
            "Epoch 173/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.7438 - val_loss: 2.0393\n",
            "Epoch 174/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.3866 - val_loss: 1.3321\n",
            "Epoch 175/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.3540 - val_loss: 1.1794\n",
            "Epoch 176/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.2906 - val_loss: 1.0564\n",
            "Epoch 177/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.2469 - val_loss: 0.8295\n",
            "Epoch 178/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.8662 - val_loss: 2.3578\n",
            "Epoch 179/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6651 - val_loss: 2.3982\n",
            "Epoch 180/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.3391 - val_loss: 1.3870\n",
            "Epoch 181/300\n",
            "28/28 [==============================] - 1s 34ms/step - loss: 3.4471 - val_loss: 1.9356\n",
            "Epoch 182/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.5877 - val_loss: 2.0387\n",
            "Epoch 183/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.4133 - val_loss: 0.8511\n",
            "Epoch 184/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4203 - val_loss: 0.6742\n",
            "Epoch 185/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.7702 - val_loss: 2.2659\n",
            "Epoch 186/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4867 - val_loss: 2.7044\n",
            "Epoch 187/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 3.4734 - val_loss: 2.2117\n",
            "Epoch 188/300\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 3.4240 - val_loss: 0.9333\n",
            "Epoch 189/300\n",
            "28/28 [==============================] - 1s 32ms/step - loss: 4.6804 - val_loss: 4.7167\n",
            "Epoch 190/300\n",
            "28/28 [==============================] - 1s 30ms/step - loss: 3.8360 - val_loss: 3.3850\n",
            "Epoch 191/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6452 - val_loss: 3.1370\n",
            "Epoch 192/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.7355 - val_loss: 4.2684\n",
            "Epoch 193/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.7303 - val_loss: 4.3493\n",
            "Epoch 194/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.2739 - val_loss: 4.6836\n",
            "Epoch 195/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.6287 - val_loss: 3.1263\n",
            "Epoch 196/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4679 - val_loss: 3.3181\n",
            "Epoch 197/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.4398 - val_loss: 4.3117\n",
            "Epoch 198/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.1739 - val_loss: 1.5595\n",
            "Epoch 199/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.2300 - val_loss: 0.6613\n",
            "Epoch 200/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 7.7226 - val_loss: 5.4563\n",
            "Epoch 201/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.3562 - val_loss: 6.6863\n",
            "Epoch 202/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 5.3065 - val_loss: 7.0920\n",
            "Epoch 203/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.4262 - val_loss: 4.4251\n",
            "Epoch 204/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.1125 - val_loss: 3.6459\n",
            "Epoch 205/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.9564 - val_loss: 2.4941\n",
            "Epoch 206/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.8623 - val_loss: 2.9360\n",
            "Epoch 207/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6001 - val_loss: 2.6044\n",
            "Epoch 208/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6266 - val_loss: 2.0238\n",
            "Epoch 209/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.1117 - val_loss: 6.5962\n",
            "Epoch 210/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 6.1177 - val_loss: 5.0039\n",
            "Epoch 211/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.9879 - val_loss: 1.2295\n",
            "Epoch 212/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.7746 - val_loss: 0.6035\n",
            "Epoch 213/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.3559 - val_loss: 1.0994\n",
            "Epoch 214/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.3784 - val_loss: 3.2052\n",
            "Epoch 215/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.3708 - val_loss: 1.8271\n",
            "Epoch 216/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.3133 - val_loss: 0.5538\n",
            "Epoch 217/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4035 - val_loss: 0.8937\n",
            "Epoch 218/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.3499 - val_loss: 3.6992\n",
            "Epoch 219/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.3082 - val_loss: 1.4499\n",
            "Epoch 220/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.5094 - val_loss: 1.1531\n",
            "Epoch 221/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.2629 - val_loss: 2.6635\n",
            "Epoch 222/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6685 - val_loss: 2.6084\n",
            "Epoch 223/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6340 - val_loss: 2.8564\n",
            "Epoch 224/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6842 - val_loss: 0.7939\n",
            "Epoch 225/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.4763 - val_loss: 0.7742\n",
            "Epoch 226/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.6466 - val_loss: 5.4960\n",
            "Epoch 227/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 4.1085 - val_loss: 3.6193\n",
            "Epoch 228/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 3.7479 - val_loss: 3.0318\n",
            "Epoch 229/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.4407 - val_loss: 1.8685\n",
            "Epoch 230/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.2367 - val_loss: 1.5015\n",
            "Epoch 231/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.1151 - val_loss: 1.2266\n",
            "Epoch 232/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.0501 - val_loss: 1.2026\n",
            "Epoch 233/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.0396 - val_loss: 0.7329\n",
            "Epoch 234/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.1024 - val_loss: 1.0162\n",
            "Epoch 235/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 3.0042 - val_loss: 0.9462\n",
            "Epoch 236/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.9835 - val_loss: 0.7903\n",
            "Epoch 237/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.9486 - val_loss: 0.8422\n",
            "Epoch 238/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.9185 - val_loss: 0.7139\n",
            "Epoch 239/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.9008 - val_loss: 0.7448\n",
            "Epoch 240/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.9117 - val_loss: 0.6418\n",
            "Epoch 241/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.8881 - val_loss: 0.6339\n",
            "Epoch 242/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 2.8788 - val_loss: 0.6536\n",
            "Epoch 243/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.8590 - val_loss: 0.6400\n",
            "Epoch 244/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.8121 - val_loss: 0.7865\n",
            "Epoch 245/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.7928 - val_loss: 0.5904\n",
            "Epoch 246/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.8262 - val_loss: 0.5486\n",
            "Epoch 247/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.8133 - val_loss: 0.7099\n",
            "Epoch 248/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.8479 - val_loss: 0.6846\n",
            "Epoch 249/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.7606 - val_loss: 0.6802\n",
            "Epoch 250/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.7406 - val_loss: 0.6390\n",
            "Epoch 251/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.7425 - val_loss: 0.7556\n",
            "Epoch 252/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.7003 - val_loss: 0.8769\n",
            "Epoch 253/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.7758 - val_loss: 1.0243\n",
            "Epoch 254/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.8119 - val_loss: 1.9316\n",
            "Epoch 255/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.7676 - val_loss: 0.8480\n",
            "Epoch 256/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 2.7207 - val_loss: 0.7411\n",
            "Epoch 257/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.7163 - val_loss: 0.7390\n",
            "Epoch 258/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.6901 - val_loss: 0.6725\n",
            "Epoch 259/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.6852 - val_loss: 0.7181\n",
            "Epoch 260/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 2.7548 - val_loss: 0.9849\n",
            "Epoch 261/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.7230 - val_loss: 0.7243\n",
            "Epoch 262/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.6873 - val_loss: 0.6925\n",
            "Epoch 263/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.6851 - val_loss: 0.8706\n",
            "Epoch 264/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.6973 - val_loss: 1.2630\n",
            "Epoch 265/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.6978 - val_loss: 0.8140\n",
            "Epoch 266/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 2.6898 - val_loss: 1.6308\n",
            "Epoch 267/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.8385 - val_loss: 0.6936\n",
            "Epoch 268/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.9340 - val_loss: 0.5769\n",
            "Epoch 269/300\n",
            "28/28 [==============================] - 1s 36ms/step - loss: 2.6849 - val_loss: 0.7506\n",
            "Epoch 270/300\n",
            "28/28 [==============================] - 1s 35ms/step - loss: 2.6254 - val_loss: 0.5812\n",
            "Epoch 271/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.6762 - val_loss: 0.7992\n",
            "Epoch 272/300\n",
            "28/28 [==============================] - 1s 34ms/step - loss: 2.6560 - val_loss: 0.7499\n",
            "Epoch 273/300\n",
            "28/28 [==============================] - 1s 34ms/step - loss: 2.6127 - val_loss: 0.8224\n",
            "Epoch 274/300\n",
            "28/28 [==============================] - 1s 30ms/step - loss: 2.6629 - val_loss: 0.6954\n",
            "Epoch 275/300\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 2.6266 - val_loss: 0.7365\n",
            "Epoch 276/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5834 - val_loss: 0.9132\n",
            "Epoch 277/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5802 - val_loss: 0.7169\n",
            "Epoch 278/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 2.6081 - val_loss: 0.8156\n",
            "Epoch 279/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5871 - val_loss: 0.7433\n",
            "Epoch 280/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5736 - val_loss: 0.7703\n",
            "Epoch 281/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.6526 - val_loss: 0.9091\n",
            "Epoch 282/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.6339 - val_loss: 0.8293\n",
            "Epoch 283/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5692 - val_loss: 0.7057\n",
            "Epoch 284/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5541 - val_loss: 0.6180\n",
            "Epoch 285/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5397 - val_loss: 0.9956\n",
            "Epoch 286/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5993 - val_loss: 1.1180\n",
            "Epoch 287/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.6306 - val_loss: 0.8896\n",
            "Epoch 288/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5716 - val_loss: 0.7625\n",
            "Epoch 289/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5628 - val_loss: 0.6438\n",
            "Epoch 290/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5557 - val_loss: 0.6197\n",
            "Epoch 291/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5847 - val_loss: 0.5812\n",
            "Epoch 292/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5607 - val_loss: 0.6776\n",
            "Epoch 293/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5229 - val_loss: 0.6420\n",
            "Epoch 294/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5244 - val_loss: 0.5988\n",
            "Epoch 295/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5542 - val_loss: 0.6062\n",
            "Epoch 296/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5449 - val_loss: 1.6900\n",
            "Epoch 297/300\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 2.6435 - val_loss: 0.6695\n",
            "Epoch 298/300\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 2.5495 - val_loss: 0.5673\n",
            "Epoch 299/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5482 - val_loss: 0.9519\n",
            "Epoch 300/300\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 2.5301 - val_loss: 0.6937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "yfQXHlHvMjdC",
        "outputId": "edaca296-e3e4-43b0-b8df-57b78fd89e15"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f72057ce650>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.predict(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "692M23T5M6Bn",
        "outputId": "6da0936c-6b27-4b8d-c0f6-8fb10f4e8c94"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[40.492397, 40.431522, 40.43411 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}